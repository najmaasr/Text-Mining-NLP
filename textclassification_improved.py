# -*- coding: utf-8 -*-
"""textclassification_improved.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aNCqKfXqftn96wKgiR7GGH_QAoJNioqf

#PERSIAPAN
"""

!pip install Wordcloud
!pip install Sastrawi

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

import re
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud, STOPWORDS
from sklearn.feature_extraction.text import CountVectorizer
from nltk.tokenize import RegexpTokenizer

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.naive_bayes import MultinomialNB

from wordcloud import WordCloud, STOPWORDS

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/Text Mining/mobil_listrik.csv')
data.head()

"""#DATA UNDERSTANDING

CEK TIPE DATA
"""

data.dtypes

"""CEK BARIS COLUMN"""

data.shape

"""Cek Missing Value"""

data.isnull().sum()

"""Hapus Missing Value"""

data = data.dropna()
data.isnull().sum()

"""Cek Duplikasi data"""

data.duplicated().sum()

"""Cek Deskripsi Data"""

data.describe()

"""#Preprocessing Text"""

data.head()

"""Hilangkan var. id_komentar"""

data = data.drop(columns= 'id_komentar')
data.head()

"""##CASE FOLDING

**Mengubah semua huruf jadi huruf kecil (lower)**
"""

#mengubah teks menjadi lower teks
data['text_cleaning'] = data['text_cleaning'].str.lower()
data.head()

"""**Menghilangkan angka dari kolom text_cleaning**"""

# Menghilangkan angka dari kolom text_cleaning
data['text_cleaning'] = data['text_cleaning'].apply(lambda x: re.sub(r'\d+', '', x))

print(data)

"""**Menghapus tanda baca**"""

# 3. Menghapus tanda baca

data['text_cleaning'] = data['text_cleaning'].apply(lambda x: re.sub(r'[^\w\s]', '', x))
print(data.head())

"""**Menghapus whitespace (karakter kosong)**"""

# 4. Menghapus whitespace (karakter kosong)
data['text_cleaning'] = data['text_cleaning'].apply(lambda x: x.strip())
print(data.head())

"""##Normalisasi"""

# Kamus normalisasi
norm = {
    "dgn": "dengan",
    "gue": "saya",
    "ngaco": "sembarangan",
    "kwalitas": "kualitas",
    "blom": "belum",
    "jt" : "juta",
    "ev" : "mobil listrik",
    "lom" : "belum"
}

# Fungsi normalisasi
def normalisasi_kata(str_text):
    for i in norm:
        str_text = str_text.replace(i, norm[i])
    return str_text



# Melakukan normalisasi pada setiap kalimat
# Contoh penggunaan:
data['text_cleaning'] = data['text_cleaning'].apply(lambda x: normalisasi_kata(x))


data.head()

"""CEK KATA YANG SUDAH DI NORMALISASI APAKAH MASIH ADA ATAU TIDAK"""

# Mencari kata 'jt' dalam kolom 'text_cleaning'
result_jt = data[data['text_cleaning'].str.contains('ev')]

# Menampilkan hasil pencarian
print(result_jt)

"""##Filtering

STOPWORD : MENGHILANGKAN KATA penghubung (dan, yang, di,..)
"""

#stopword
import Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, StopWordRemover,ArrayDictionary

more_stop_word = []

# Mendapatkan daftar stop words
stop_words = StopWordRemoverFactory().get_stop_words()

# Membuat sebuah array baru dari daftar stop words
new_array = ArrayDictionary(stop_words)

# Membuat objek StopWordRemover dengan array baru
stop_words_remorver_new = StopWordRemover(new_array)

# Fungsi untuk menghapus stop words dari teks
def stopword(str_text):
  str_text = stop_words_remorver_new.remove(str_text)
  return str_text

# Menggunakan fungsi untuk membersihkan kolom 'text_cleaning' di DataFrame 'data'
data['text_cleaning'] = data['text_cleaning'].apply(lambda x: stopword(x))
data.head()

"""##Tokenisasi"""

tokenized = data['text_cleaning'].apply(lambda x:x.split())
tokenized

from collections import Counter
import matplotlib.pyplot as plt


# Menggabungkan semua list token menjadi satu list
all_words = [word for sentence in tokenized for word in sentence]

# Menghitung frekuensi kemunculan kata
word_freq = Counter(all_words)

# Mengambil kata-kata yang paling sering muncul
common_words = word_freq.most_common(30)  # Ubah 10 sesuai kebutuhan

# Memisahkan kata dan frekuensinya
words, frequencies = zip(*common_words)

# Membuat plot
plt.figure(figsize=(10, 6))
plt.bar(words, frequencies)
plt.xlabel('Kata')
plt.ylabel('Frekuensi')
plt.title('Kata yang Paling Sering Muncul')
plt.xticks(rotation=45)
plt.show()

"""##Stemming (Merubah kata menjadi kata dasar)"""

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

def stemming(text_cleaning):
  factory = StemmerFactory()
  stemmer = factory.create_stemmer()
  do = []
  for w in text_cleaning:
    dt = stemmer.stem(w)
    do.append(dt)
    do.append(' ')  # Tambahkan spasi

  d_clean = []
  d_clean = "".join(do)
  print(d_clean)
  return d_clean

tokenized = tokenized.apply(stemming)

# Mencari kata 'jt' dalam data setelah stemming
result_jt = [text for text in tokenized if 'ev' in text]

# Menampilkan hasil pencarian
print(result_jt)

"""Simpan kedalam CSV"""

tokenized.to_csv('/content/drive/MyDrive/Text Mining/mobil_clean1.csv', index = False)

"""#Pembuatan Model (bisa mulai dari sini kalo mau testing)

Panggil data yang sudah di cleaning
"""

data_clean = pd.read_csv('/content/drive/MyDrive/Text Mining/mobil_clean1.csv')
data_clean.head()

"""Penggabungan 2 Atribut Data"""

atribut2 = pd.read_csv('/content/drive/MyDrive/Text Mining/mobil_listrik.csv')
atribut1 = pd.read_csv('/content/drive/MyDrive/Text Mining/mobil_clean1.csv')

att2 = atribut2['sentimen']

data_clean = pd.concat((atribut1, att2), axis = 1)
data_clean.head()

data_clean

"""Menghapus baris dengan nilai teks yang hilang"""

data_clean = data_clean.dropna()

import matplotlib.pyplot as plt

# Menghitung jumlah data dalam setiap kategori sentimen
sentimen_counts = data_clean['sentimen'].value_counts()

# Menyiapkan data untuk visualisasi
labels = sentimen_counts.index
values = sentimen_counts.values

# Membuat pie chart
plt.figure(figsize=(8, 6))
plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)
plt.title('Jumlah Data dalam Setiap Kategori Sentimen')
plt.axis('equal')  # Agar pie chart menjadi lingkaran
plt.show()

"""Menduplikasi data berdasarkan 2 kondisi"""

# DataFrame dengan nilai netral
data_clean_full= data_clean.copy()

import matplotlib.pyplot as plt

# Menghitung jumlah data dalam setiap kategori sentimen
sentimen_counts = data_clean_full['sentimen'].value_counts()

# Menyiapkan data untuk visualisasi
labels = sentimen_counts.index
values = sentimen_counts.values

# Membuat pie chart
plt.figure(figsize=(8, 6))
plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)
plt.title('Jumlah Data dalam Setiap Kategori Sentimen')
plt.axis('equal')  # Agar pie chart menjadi lingkaran
plt.show()

# DataFrame tanpa nilai netral
data_clean_noneutral = data_clean[data_clean['sentimen'] != 'netral'].copy()

import matplotlib.pyplot as plt

# Menghitung jumlah data dalam setiap kategori sentimen
sentimen_counts = data_clean_noneutral['sentimen'].value_counts()

# Menyiapkan data untuk visualisasi
labels = sentimen_counts.index
values = sentimen_counts.values

# Membuat pie chart
plt.figure(figsize=(8, 6))
plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)
plt.title('Jumlah Data dalam Setiap Kategori Sentimen')
plt.axis('equal')  # Agar pie chart menjadi lingkaran
plt.show()

"""##Transformasi

Karena akan menggunakan naive bayes, maka value harus diubah ke 0 dan 1
"""

data_clean_noneutral = data_clean_noneutral.replace({'positif' :1, 'negatif' :0})
data_clean_noneutral.head()

"""##VISUALISASI KATA"""

data_negatif = data_clean_noneutral [data_clean_noneutral['sentimen'] == 0]
data_positif = data_clean_noneutral [data_clean_noneutral['sentimen'] == 1]

"""###VISUALISASI KATA NEGATIF"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Menggabungkan semua teks yang memiliki sentimen negatif menjadi satu teks
teks_negatif = ' '.join(data_negatif['text_cleaning'].tolist())

# Pra-pemrosesan teks (opsional: sesuaikan dengan kebutuhan Anda)
# Contoh pra-pemrosesan: tokenisasi dan penghapusan stopwords
# stopwords_indonesia = set(stopwords.words('indonesian'))
# words = word_tokenize(teks_negatif.lower())
# words = [word for word in words if word not in stopwords_indonesia]

# Buat Word Cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(teks_negatif)

# Tampilkan Word Cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud Kata-kata Bernilai Negatif')
plt.show()

"""###VISUALISASI KATA POSITIF"""

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

# Menggabungkan semua teks yang memiliki sentimen negatif menjadi satu teks
teks_positif = ' '.join(data_positif['text_cleaning'].tolist())

# Pra-pemrosesan teks (opsional: sesuaikan dengan kebutuhan Anda)
# Contoh pra-pemrosesan: tokenisasi dan penghapusan stopwords
# stopwords_indonesia = set(stopwords.words('indonesian'))
# words = word_tokenize(teks_negatif.lower())
# words = [word for word in words if word not in stopwords_indonesia]

# Buat Word Cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(teks_positif)

# Tampilkan Word Cloud
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud Kata-kata Bernilai Positif')
plt.show()

"""Menghitung jumlah kata positif dan negatif"""

data_clean_noneutral['sentimen'].value_counts()

"""###Visualisasi pada nilai sentiment"""

import matplotlib.pyplot as plt

# Menghitung jumlah sentimen positif dan negatif
jumlah_sentimen = data_clean_noneutral['sentimen'].value_counts()

# Membuat bar chart
plt.figure(figsize=(6, 6))
plt.bar(jumlah_sentimen.index, jumlah_sentimen.values, color=['green', 'red'])

# Memberi label pada sumbu x dan y
plt.xlabel('Sentimen')
plt.ylabel('Jumlah')
plt.title('Jumlah Sentimen Positif dan Negatif')

# Memberi label pada sumbu x untuk nilai sentimen
plt.xticks([0, 1], ['Negatif', 'Positif'])

# Menampilkan nilai di atas bar chart
for i, value in enumerate(jumlah_sentimen.values):
    plt.text(i, value + 5, str(value), ha='center')

# Menampilkan bar chart
plt.show()

"""#Data Prep

**SPLIT DATA**
"""

from sklearn.model_selection import train_test_split

# Memisahkan fitur (text_cleaning) dan target (sentimen)
X = data_clean_noneutral['text_cleaning']
y = data_clean_noneutral['sentimen']

# Memisahkan data menjadi bagian pelatihan (85%) dan pengujian (15%)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)

# Menampilkan jumlah data pada setiap bagian
print("Jumlah data pada data pelatihan:", len(X_train))
print("Jumlah data pada data pengujian:", len(X_test))

"""##Mengubah nilai teks menjadi angka

kedua CountVectorizer dan TfidfVectorizer memiliki fungsi untuk mengubah kata-kata menjadi representasi angka agar bisa digunakan dalam model machine learning. Proses ini dikenal sebagai vektorisasi teks. Mereka mengonversi teks menjadi representasi numerik yang bisa dipahami oleh algoritma machine learning.

##Menggunakan count vectorizer (tutorial yt)
"""

from sklearn.feature_extraction.text import CountVectorizer

# Inisialisasi objek CountVectorizer
vectorizer = CountVectorizer()

# Melakukan vektorisasi pada data pelatihan dan pengujian
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

# Contoh: Melihat dimensi vektor setelah vektorisasi
print("Dimensi vektor X_train setelah vektorisasi:", X_train_vec.shape)
print("Dimensi vektor X_test setelah vektorisasi:", X_test_vec.shape)

#Melakukan oversampling menggunakan SMOTE pada data latih
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vec, y_train)

import matplotlib.pyplot as plt

# Menghitung jumlah sentimen positif dan negatif
jumlah_sentimen = y_train_resampled.value_counts()

# Membuat bar chart
plt.figure(figsize=(6, 6))
plt.bar(jumlah_sentimen.index, jumlah_sentimen.values, color=['green', 'red'])

# Memberi label pada sumbu x dan y
plt.xlabel('Sentimen')
plt.ylabel('Jumlah')
plt.title('Visualisasi Sentimen menggunakan SMOTE')

# Memberi label pada sumbu x untuk nilai sentimen
plt.xticks(jumlah_sentimen.index, [0, 1])



# Menampilkan bar chart
plt.show()

"""Modeling"""

naive_bayes = MultinomialNB()
naive_bayes.fit(X_train_resampled, y_train_resampled)

y_pred = naive_bayes.predict(X_test_vec)

"""EVALUASI MODEL"""

from sklearn.metrics import accuracy_score, classification_report  # Pastikan untuk mengimpor classification_report



# Evaluasi Model
accuracy = accuracy_score(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred, target_names=['negatif', 'positif'])

print("Akurasi Model naive Bayes: ", accuracy)
print("\nLaporan Klasifikasi:\n", classification_rep)  # Menggunakan variabel classification_rep

"""##Menggunakan TDIF vectorizer (Pakai ini untuk Case ini)"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi objek TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()

# Melakukan vektorisasi pada data pelatihan dan pengujian
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
X_test_tfidf = tfidf_vectorizer.transform(X_test)

# Contoh: Melihat dimensi vektor setelah vektorisasi
print("Dimensi vektor X_train setelah vektorisasi:", X_train_tfidf.shape)
print("Dimensi vektor X_test setelah vektorisasi:", X_test_tfidf.shape)

"""menangani oversampling menggunakan SMOTE

**SMOTE (Synthetic Minority Over-sampling Technique)** adalah salah satu metode oversampling yang digunakan dalam penanganan ketidakseimbangan kelas pada dataset. Oversampling adalah teknik yang digunakan untuk menangani situasi di mana kelas mayoritas memiliki lebih banyak sampel daripada kelas minoritas dalam dataset. SMOTE khususnya digunakan untuk menangani ketidakseimbangan kelas dengan cara menciptakan sampel sintetis dari kelas minoritas untuk menyamakan jumlah sampel antara kelas mayoritas dan minoritas.
"""

from imblearn.over_sampling import SMOTE

# Inisialisasi objek SMOTE
smote = SMOTE(random_state=42)

# Melakukan oversampling pada data latih
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)

# Menggunakan shape untuk mendapatkan jumlah baris (sampel) setelah SMOTE
print("Jumlah sampel setelah penerapan SMOTE:", X_train_resampled.shape[0])

import matplotlib.pyplot as plt

# Menghitung jumlah sentimen positif dan negatif
jumlah_sentimen = y_train_resampled.value_counts()

# Membuat bar chart
plt.figure(figsize=(6, 6))
plt.bar(jumlah_sentimen.index, jumlah_sentimen.values, color=['green', 'red'])

# Memberi label pada sumbu x dan y
plt.xlabel('Sentimen')
plt.ylabel('Jumlah')
plt.title('Visualisasi Sentimen menggunakan SMOTE')

# Memberi label pada sumbu x untuk nilai sentimen
plt.xticks(jumlah_sentimen.index, [0, 1])



# Menampilkan bar chart
plt.show()

"""**Modeling**"""

naive_bayes = MultinomialNB()
naive_bayes.fit(X_train_resampled, y_train_resampled)

y_pred = naive_bayes.predict(X_test_tfidf)

"""**Evaluasi Model**"""

from sklearn.metrics import accuracy_score, classification_report

#Evaluasi Model
accuracy = accuracy_score(y_test, y_pred)
classification_report = classification_report(y_test, y_pred, target_names = ['negatif', 'positif'])

print("Akurasi Model naive Bayes : ", accuracy)
print("\nLaporan Klasifikasi :\n ", classification_report)

"""**Confusion Matrix**"""

from sklearn.metrics import confusion_matrix

# Misal y_test adalah nilai sebenarnya dan y_pred adalah nilai yang diprediksi oleh model
conf_matrix = confusion_matrix(y_test, y_pred)

print("Confusion Matrix:")
print(conf_matrix)

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Menghitung confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Visualisasi confusion matrix menggunakan heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=['negatif', 'positif'], yticklabels=['negatif', 'positif'])
plt.xlabel('Prediksi')
plt.ylabel('Aktual')
plt.title('Confusion Matrix')
plt.show()

"""**Testing**"""

new_text = input("\nMasukan teks baru : ")
new_text_vec = tfidf_vectorizer.transform([new_text]) #parameter ini diambil pada saat inisiasi dgn tdif vectorizer
predicted_sentimen = naive_bayes.predict(new_text_vec)

if predicted_sentimen[0] ==1 :
  sentiment_label ='positif'
elif predicted_sentimen[0] ==0 :
  sentiment_label ='negatif'

print("Hasil Analisis Sentimen untuk Teks Baru : ", sentiment_label)